{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdfeb11c",
   "metadata": {},
   "source": [
    "### âœ… Step 1: Transcribe Audio Using Whisper ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7f61a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cairuser1/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "Model loaded on devices: [0, 1]\n",
      "Sample 0 (Speech Status: dysarthria, Gender: male):\n",
      "  Duration: 3.16 seconds\n",
      "  Original Transcript: storm\n",
      "  ASR Transcript:  It's done.\n",
      "--------------------------------------------------\n",
      "Sample 1 (Speech Status: dysarthria, Gender: female):\n",
      "  Duration: 4.45 seconds\n",
      "  Original Transcript: She wore warm fleecy woolen overalls\n",
      "  ASR Transcript:  Okay. She wore a white fleece brown overalls.\n",
      "--------------------------------------------------\n",
      "Sample 2 (Speech Status: dysarthria, Gender: female):\n",
      "  Duration: 1.35 seconds\n",
      "  Original Transcript: single\n",
      "  ASR Transcript:  single\n",
      "--------------------------------------------------\n",
      "Sample 3 (Speech Status: dysarthria, Gender: male):\n",
      "  Duration: 2.11 seconds\n",
      "  Original Transcript: alpha\n",
      "  ASR Transcript:  Alpha\n",
      "--------------------------------------------------\n",
      "Sample 4 (Speech Status: dysarthria, Gender: male):\n",
      "  Duration: 3.0 seconds\n",
      "  Original Transcript: sip\n",
      "  ASR Transcript:  Sip.\n",
      "--------------------------------------------------\n",
      "Sample 5 (Speech Status: dysarthria, Gender: female):\n",
      "  Duration: 1.05 seconds\n",
      "  Original Transcript: yes\n",
      "  ASR Transcript:  Yeah.\n",
      "--------------------------------------------------\n",
      "Sample 6 (Speech Status: dysarthria, Gender: female):\n",
      "  Duration: 2.7 seconds\n",
      "  Original Transcript: mitten\n",
      "  ASR Transcript:  Mitten? Mitten?\n",
      "--------------------------------------------------\n",
      "Sample 7 (Speech Status: dysarthria, Gender: male):\n",
      "  Duration: 8.36 seconds\n",
      "  Original Transcript: Well he is nearly ninetythree years old\n",
      "  ASR Transcript:  Why? He is really 93 years old.\n",
      "--------------------------------------------------\n",
      "Sample 8 (Speech Status: dysarthria, Gender: female):\n",
      "  Duration: 6.06 seconds\n",
      "  Original Transcript: It eventually became a rather proud claim\n",
      "  ASR Transcript:  It eventually became a rather proud claim.\n",
      "--------------------------------------------------\n",
      "Sample 9 (Speech Status: dysarthria, Gender: male):\n",
      "  Duration: 3.54 seconds\n",
      "  Original Transcript: He wrapped the package hastily\n",
      "  ASR Transcript:  He wrapped the package hastily.\n",
      "--------------------------------------------------\n",
      "Sample 10 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 1.65 seconds\n",
      "  Original Transcript: five\n",
      "  ASR Transcript:  Bye.\n",
      "--------------------------------------------------\n",
      "Sample 11 (Speech Status: healthy, Gender: male):\n",
      "  Duration: 1.2 seconds\n",
      "  Original Transcript: fill\n",
      "  ASR Transcript:  Phil.\n",
      "--------------------------------------------------\n",
      "Sample 12 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 3.75 seconds\n",
      "  Original Transcript: know\n",
      "  ASR Transcript:  No.\n",
      "--------------------------------------------------\n",
      "Sample 13 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 2.7 seconds\n",
      "  Original Transcript: lock\n",
      "  ASR Transcript:  luck.\n",
      "--------------------------------------------------\n",
      "Sample 14 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 2.4 seconds\n",
      "  Original Transcript: up\n",
      "  ASR Transcript:  Up.\n",
      "--------------------------------------------------\n",
      "Sample 15 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 1.66 seconds\n",
      "  Original Transcript: eyes\n",
      "  ASR Transcript:  Eyes.\n",
      "--------------------------------------------------\n",
      "Sample 16 (Speech Status: healthy, Gender: female):\n",
      "  Duration: 2.04 seconds\n",
      "  Original Transcript: chop\n",
      "  ASR Transcript:  Chop.\n",
      "--------------------------------------------------\n",
      "Sample 17 (Speech Status: healthy, Gender: male):\n",
      "  Duration: 2.64 seconds\n",
      "  Original Transcript: seed\n",
      "  ASR Transcript:  Seed.\n",
      "--------------------------------------------------\n",
      "Sample 18 (Speech Status: healthy, Gender: male):\n",
      "  Duration: 2.85 seconds\n",
      "  Original Transcript: The wait for work can be very long\n",
      "  ASR Transcript:  The wait for work can be very long.\n",
      "--------------------------------------------------\n",
      "Sample 19 (Speech Status: healthy, Gender: male):\n",
      "  Duration: 1.47 seconds\n",
      "  Original Transcript: pit\n",
      "  ASR Transcript:  Pit.\n",
      "--------------------------------------------------\n",
      "Results in JSON format:\n",
      "{\n",
      "    \"sample_0\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 3.16,\n",
      "        \"original_transcript\": \"storm\",\n",
      "        \"asr_transcript\": \" It's done.\"\n",
      "    },\n",
      "    \"sample_1\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 4.45,\n",
      "        \"original_transcript\": \"She wore warm fleecy woolen overalls\",\n",
      "        \"asr_transcript\": \" Okay. She wore a white fleece brown overalls.\"\n",
      "    },\n",
      "    \"sample_2\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 1.35,\n",
      "        \"original_transcript\": \"single\",\n",
      "        \"asr_transcript\": \" single\"\n",
      "    },\n",
      "    \"sample_3\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 2.11,\n",
      "        \"original_transcript\": \"alpha\",\n",
      "        \"asr_transcript\": \" Alpha\"\n",
      "    },\n",
      "    \"sample_4\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 3.0,\n",
      "        \"original_transcript\": \"sip\",\n",
      "        \"asr_transcript\": \" Sip.\"\n",
      "    },\n",
      "    \"sample_5\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 1.05,\n",
      "        \"original_transcript\": \"yes\",\n",
      "        \"asr_transcript\": \" Yeah.\"\n",
      "    },\n",
      "    \"sample_6\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 2.7,\n",
      "        \"original_transcript\": \"mitten\",\n",
      "        \"asr_transcript\": \" Mitten? Mitten?\"\n",
      "    },\n",
      "    \"sample_7\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 8.36,\n",
      "        \"original_transcript\": \"Well he is nearly ninetythree years old\",\n",
      "        \"asr_transcript\": \" Why? He is really 93 years old.\"\n",
      "    },\n",
      "    \"sample_8\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 6.06,\n",
      "        \"original_transcript\": \"It eventually became a rather proud claim\",\n",
      "        \"asr_transcript\": \" It eventually became a rather proud claim.\"\n",
      "    },\n",
      "    \"sample_9\": {\n",
      "        \"speech_status\": \"dysarthria\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 3.54,\n",
      "        \"original_transcript\": \"He wrapped the package hastily\",\n",
      "        \"asr_transcript\": \" He wrapped the package hastily.\"\n",
      "    },\n",
      "    \"sample_10\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 1.65,\n",
      "        \"original_transcript\": \"five\",\n",
      "        \"asr_transcript\": \" Bye.\"\n",
      "    },\n",
      "    \"sample_11\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 1.2,\n",
      "        \"original_transcript\": \"fill\",\n",
      "        \"asr_transcript\": \" Phil.\"\n",
      "    },\n",
      "    \"sample_12\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 3.75,\n",
      "        \"original_transcript\": \"know\",\n",
      "        \"asr_transcript\": \" No.\"\n",
      "    },\n",
      "    \"sample_13\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 2.7,\n",
      "        \"original_transcript\": \"lock\",\n",
      "        \"asr_transcript\": \" luck.\"\n",
      "    },\n",
      "    \"sample_14\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 2.4,\n",
      "        \"original_transcript\": \"up\",\n",
      "        \"asr_transcript\": \" Up.\"\n",
      "    },\n",
      "    \"sample_15\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 1.66,\n",
      "        \"original_transcript\": \"eyes\",\n",
      "        \"asr_transcript\": \" Eyes.\"\n",
      "    },\n",
      "    \"sample_16\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"female\",\n",
      "        \"duration_seconds\": 2.04,\n",
      "        \"original_transcript\": \"chop\",\n",
      "        \"asr_transcript\": \" Chop.\"\n",
      "    },\n",
      "    \"sample_17\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 2.64,\n",
      "        \"original_transcript\": \"seed\",\n",
      "        \"asr_transcript\": \" Seed.\"\n",
      "    },\n",
      "    \"sample_18\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 2.85,\n",
      "        \"original_transcript\": \"The wait for work can be very long\",\n",
      "        \"asr_transcript\": \" The wait for work can be very long.\"\n",
      "    },\n",
      "    \"sample_19\": {\n",
      "        \"speech_status\": \"healthy\",\n",
      "        \"gender\": \"male\",\n",
      "        \"duration_seconds\": 1.47,\n",
      "        \"original_transcript\": \"pit\",\n",
      "        \"asr_transcript\": \" Pit.\"\n",
      "    }\n",
      "}\n",
      "Results saved to 'transcription_results.json'\n",
      "Transcription completed.\n",
      "CUDA memory released.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torchaudio\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "\n",
    "# Set environment variable to avoid memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 1. Load TORGO dataset\n",
    "dataset = load_dataset(\"abnerh/TORGO-database\")\n",
    "\n",
    "# 2. Filter samples by speaker type\n",
    "def filter_by_speaker_type(data, speaker_prefix):\n",
    "    return [item for item in data if item['speech_status'].startswith(speaker_prefix)]\n",
    "\n",
    "dysarthric_data = filter_by_speaker_type(dataset['train'], 'dysarthria')\n",
    "healthy_data = filter_by_speaker_type(dataset['train'], 'healthy')\n",
    "\n",
    "# 3. Randomly select 10 from each category\n",
    "selected_dysarthric = random.sample(dysarthric_data, 10)\n",
    "selected_healthy = random.sample(healthy_data, 10)\n",
    "selected_samples = selected_dysarthric + selected_healthy\n",
    "\n",
    "# 4. Check GPU availability\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "assert torch.cuda.device_count() >= 2, \"Requires at least 2 GPUs.\"\n",
    "\n",
    "# Clear GPU memory before loading the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 5. Load Whisper large-v2 model and set up DataParallel\n",
    "try:\n",
    "    base_model = whisper.load_model(\"large-v2\")  # Try large-v2 first\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"Large-v2 model too big for GPU memory. Falling back to 'medium'.\")\n",
    "    base_model = whisper.load_model(\"medium\")  # Fallback to medium if OOM\n",
    "\n",
    "# Define device_ids explicitly\n",
    "device_ids = [0, 1]  # Use GPU 0 and GPU 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "asr_model = torch.nn.DataParallel(base_model, device_ids=device_ids).to(device)\n",
    "print(f\"Model loaded on devices: {device_ids}\")\n",
    "\n",
    "# 6. Whisper sample rate\n",
    "WHISPER_SR = 16000\n",
    "\n",
    "# 7. Transcription function\n",
    "def transcribe_whisper_array(audio_array, original_sr):\n",
    "    audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "\n",
    "    # Resample if sampling rate differs\n",
    "    if original_sr != WHISPER_SR:\n",
    "        resampler = T.Resample(orig_freq=original_sr, new_freq=WHISPER_SR)\n",
    "        audio_tensor = resampler(audio_tensor)\n",
    "\n",
    "    # Ensure mono\n",
    "    if audio_tensor.ndim > 1:\n",
    "        audio_tensor = torch.mean(audio_tensor, dim=0)\n",
    "\n",
    "    # Convert to float32 numpy array\n",
    "    audio_np = audio_tensor.numpy().astype(np.float32)\n",
    "\n",
    "    # Transcribe with Whisper\n",
    "    result = asr_model.module.transcribe(audio_np, language='en')\n",
    "    \n",
    "    # Clean up tensors within the function\n",
    "    del audio_tensor, audio_np\n",
    "    torch.cuda.empty_cache()\n",
    "    return result[\"text\"]\n",
    "\n",
    "# 8. Transcribe and collect results\n",
    "retrieved_samples = {}\n",
    "\n",
    "for idx, sample in enumerate(selected_samples):\n",
    "    try:\n",
    "        audio_array = sample[\"audio\"][\"array\"]\n",
    "        sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "        asr_transcript = transcribe_whisper_array(audio_array, sampling_rate)\n",
    "        \n",
    "        # Calculate duration in seconds\n",
    "        duration = len(audio_array) / sampling_rate\n",
    "        \n",
    "        # Get original transcription if available\n",
    "        transcription = sample.get(\"transcription\", \"No original transcription available\")\n",
    "        \n",
    "        # Get speech status and gender\n",
    "        speech_status = sample[\"speech_status\"]\n",
    "        gender = sample.get(\"gender\", \"Unknown\")  # Fallback if gender field is missing\n",
    "        \n",
    "        # Store results in a structured dictionary\n",
    "        retrieved_samples[f\"sample_{idx}\"] = {\n",
    "            \"speech_status\": speech_status,\n",
    "            \"gender\": gender,\n",
    "            \"duration_seconds\": round(duration, 2),  # Rounded to 2 decimal places\n",
    "            \"original_transcript\": transcription,\n",
    "            \"asr_transcript\": asr_transcript\n",
    "        }\n",
    "        \n",
    "        # Print for console feedback (optional)\n",
    "        print(f\"Sample {idx} (Speech Status: {speech_status}, Gender: {gender}):\")\n",
    "        print(f\"  Duration: {round(duration, 2)} seconds\")\n",
    "        print(f\"  Original Transcript: {transcription}\")\n",
    "        print(f\"  ASR Transcript: {asr_transcript}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(f\"Out of memory at sample {idx}. Clearing cache and retrying.\")\n",
    "        torch.cuda.empty_cache()\n",
    "        asr_transcript = transcribe_whisper_array(audio_array, sampling_rate)\n",
    "        \n",
    "        # Calculate duration in seconds\n",
    "        duration = len(audio_array) / sampling_rate\n",
    "        \n",
    "        # Retry with same output logic\n",
    "        transcription = sample.get(\"transcription\", \"No original transcription available\")\n",
    "        speech_status = sample[\"speech_status\"]\n",
    "        gender = sample.get(\"gender\", \"Unknown\")\n",
    "        retrieved_samples[f\"sample_{idx}\"] = {\n",
    "            \"speech_status\": speech_status,\n",
    "            \"gender\": gender,\n",
    "            \"duration_seconds\": round(duration, 2),\n",
    "            \"original_transcript\": transcription,\n",
    "            \"asr_transcript\": asr_transcript\n",
    "        }\n",
    "        \n",
    "        print(f\"Sample {idx} (Speech Status: {speech_status}, Gender: {gender}) after retry:\")\n",
    "        print(f\"  Duration: {round(duration, 2)} seconds\")\n",
    "        print(f\"  Original Transcript: {transcription}\")\n",
    "        print(f\"  ASR Transcript: {asr_transcript}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    finally:\n",
    "        # Clear GPU memory after each sample\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# 9. Convert results to JSON and save/output\n",
    "json_output = json.dumps(retrieved_samples, indent=4)\n",
    "print(\"Results in JSON format:\")\n",
    "print(json_output)\n",
    "\n",
    "# Optionally save to a file\n",
    "with open(\"transcription_results.json\", \"w\") as f:\n",
    "    f.write(json_output)\n",
    "print(\"Results saved to 'transcription_results.json'\")\n",
    "\n",
    "# 10. Clean up after transcription is complete\n",
    "print(\"Transcription completed.\")\n",
    "del asr_model, base_model  # Explicitly delete model objects\n",
    "torch.cuda.empty_cache()  # Clear cache one last time\n",
    "gc.collect()  # Final garbage collection\n",
    "print(\"CUDA memory released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16502d6",
   "metadata": {},
   "source": [
    "### âœ… Step 2: Text to IPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c447a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:46:25.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 16:46:25.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 16:46:25.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11169 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3060) - 11832 MiB free\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 46.22 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 8\n",
      "print_info: n_expert_used    = 2\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8x7B\n",
      "print_info: model params     = 46.70 B\n",
      "print_info: general.name     = mistralai_mixtral-8x7b-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA1\n",
      "load_tensors: layer  18 assigned to device CUDA1\n",
      "load_tensors: layer  19 assigned to device CUDA1\n",
      "load_tensors: layer  20 assigned to device CUDA1\n",
      "load_tensors: layer  21 assigned to device CUDA1\n",
      "load_tensors: layer  22 assigned to device CUDA1\n",
      "load_tensors: layer  23 assigned to device CUDA1\n",
      "load_tensors: layer  24 assigned to device CUDA1\n",
      "load_tensors: layer  25 assigned to device CUDA1\n",
      "load_tensors: layer  26 assigned to device CUDA1\n",
      "load_tensors: layer  27 assigned to device CUDA1\n",
      "load_tensors: layer  28 assigned to device CUDA1\n",
      "load_tensors: layer  29 assigned to device CUDA1\n",
      "load_tensors: layer  30 assigned to device CUDA1\n",
      "load_tensors: layer  31 assigned to device CUDA1\n",
      "load_tensors: layer  32 assigned to device CUDA1\n",
      "llama_model_load: error loading model: missing tensor 'blk.0.ffn_down_exps.weight'\n",
      "llama_model_load_from_file_impl: failed to load model\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060) - 11173 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3060) - 11832 MiB free\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 995 tensors from /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:  898 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 46.22 GiB (8.50 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 8\n",
      "print_info: n_expert_used    = 2\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8x7B\n",
      "print_info: model params     = 46.70 B\n",
      "print_info: general.name     = mistralai_mixtral-8x7b-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA1\n",
      "load_tensors: layer  18 assigned to device CUDA1\n",
      "load_tensors: layer  19 assigned to device CUDA1\n",
      "load_tensors: layer  20 assigned to device CUDA1\n",
      "load_tensors: layer  21 assigned to device CUDA1\n",
      "load_tensors: layer  22 assigned to device CUDA1\n",
      "load_tensors: layer  23 assigned to device CUDA1\n",
      "load_tensors: layer  24 assigned to device CUDA1\n",
      "load_tensors: layer  25 assigned to device CUDA1\n",
      "load_tensors: layer  26 assigned to device CUDA1\n",
      "load_tensors: layer  27 assigned to device CUDA1\n",
      "load_tensors: layer  28 assigned to device CUDA1\n",
      "load_tensors: layer  29 assigned to device CUDA1\n",
      "load_tensors: layer  30 assigned to device CUDA1\n",
      "load_tensors: layer  31 assigned to device CUDA1\n",
      "load_tensors: layer  32 assigned to device CUDA1\n",
      "llama_model_load: error loading model: missing tensor 'blk.0.ffn_down_exps.weight'\n",
      "llama_model_load_from_file_impl: failed to load model\n",
      "2025-04-09 16:46:25.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 16:46:25.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf\n",
      "Model loading failed: Failed to load model from file: /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf\n",
      "Model cleaned up.\n",
      "Retrying after cleanup...\n",
      "Permanent model loading failure: Failed to load model from file: /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf\n",
      "LLaMA model loaded for IPA generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m asr_transcript \u001b[38;5;241m=\u001b[39m sample_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masr_transcript\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Generate IPA\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m original_ipa \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ipa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_transcript\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m original_transcript \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo original transcription available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m asr_ipa \u001b[38;5;241m=\u001b[39m generate_ipa(llama_model, asr_transcript)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Add IPA to the sample data\u001b[39;00m\n",
      "File \u001b[0;32m~/lip/process_text.py:75\u001b[0m, in \u001b[0;36mgenerate_ipa\u001b[0;34m(model, text)\u001b[0m\n\u001b[1;32m     60\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert phonetician. Convert this text to International Phonetic Alphabets (IPA):\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124mText: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mIPA:\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):  \u001b[38;5;66;03m# 2 attempts\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m(\n\u001b[1;32m     76\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m     77\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     78\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     79\u001b[0m             stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText:\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     80\u001b[0m         )\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     82\u001b[0m         ipa_start \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from process_text import load_model, generate_ipa\n",
    "import gc\n",
    "\n",
    "# Load the LLaMA model for IPA generation\n",
    "llama_model = load_model(\"/media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/mixtral-8x7b-v0.1.Q8_0.gguf\")\n",
    "print(\"LLaMA model loaded for IPA generation.\")\n",
    "\n",
    "# Load the existing JSON file\n",
    "input_json_file = \"transcription_results.json\"  # Your existing file from Whisper\n",
    "with open(input_json_file, \"r\") as f:\n",
    "    retrieved_samples = json.load(f)\n",
    "\n",
    "# Process each sample to add IPA\n",
    "for sample_key, sample_data in retrieved_samples.items():\n",
    "    # Extract transcripts\n",
    "    original_transcript = sample_data[\"original_transcript\"]\n",
    "    asr_transcript = sample_data[\"asr_transcript\"]\n",
    "    \n",
    "    # Generate IPA\n",
    "    original_ipa = generate_ipa(llama_model, original_transcript) if original_transcript != \"No original transcription available\" else \"N/A\"\n",
    "    asr_ipa = generate_ipa(llama_model, asr_transcript)\n",
    "    \n",
    "    # Add IPA to the sample data\n",
    "    sample_data[\"original_ipa\"] = original_ipa\n",
    "    sample_data[\"asr_ipa\"] = asr_ipa\n",
    "    \n",
    "    # Optional: Print for feedback\n",
    "    print(f\"Processed {sample_key}:\")\n",
    "    print(f\"  Original Transcript: {original_transcript}\")\n",
    "    print(f\"  ASR Transcript: {asr_transcript}\")\n",
    "    print(f\"  Original IPA: {original_ipa}\")\n",
    "    print(f\"  ASR IPA: {asr_ipa}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the updated JSON to a new file\n",
    "output_json_file = \"transcription_results_with_ipa.json\"\n",
    "json_output = json.dumps(retrieved_samples, indent=4)\n",
    "with open(output_json_file, \"w\") as f:\n",
    "    f.write(json_output)\n",
    "print(f\"Updated results saved to '{output_json_file}'\")\n",
    "\n",
    "# Clean up\n",
    "del llama_model\n",
    "gc.collect()\n",
    "print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f25b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from process_text import load_model, generate_ipa,evaluate_transcriptions\n",
    "import gc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad536ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the three model paths\n",
    "model_paths = [\n",
    "    \"/media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/llama-chat-3.1-q8.gguf\",  # Model 1\n",
    "    \"/media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/llama-2-7b-chat.Q8_0.gguf\",  # Model 2\n",
    "    \"/media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/Ministral-8B-Instruct-2410-Q8_0.gguf\"  # Model 3\n",
    "]\n",
    "\n",
    "# Load the existing JSON file\n",
    "input_json_file = \"transcription_results.json\"\n",
    "print(f\"Loading input JSON from {input_json_file}\")\n",
    "with open(input_json_file, \"r\") as f:\n",
    "    retrieved_samples = json.load(f)\n",
    "print(\"Input JSON loaded successfully.\")\n",
    "\n",
    "# Process each model sequentially\n",
    "for i, model_path in enumerate(model_paths, 1):\n",
    "    model_name = f\"model_{i}\"\n",
    "    is_mixtral = \"mixtral\" in model_path.lower()\n",
    "    print(f\"\\nStarting processing for {model_name} from {model_path}\")\n",
    "    \n",
    "    # Check if the model file exists and its size\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file {model_path} does not exist.\")\n",
    "        continue\n",
    "    file_size = os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"Model file {model_path} found, size: {file_size:.2f} MB\")\n",
    "    \n",
    "    # Load the model with error handling\n",
    "    try:\n",
    "        model = load_model(model_path, is_mixtral=is_mixtral)\n",
    "        if model is None:\n",
    "            raise ValueError(f\"load_model returned None for {model_path}\")\n",
    "        print(f\"{model_name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {model_name} from {model_path}: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Process each sample with the current model\n",
    "    for sample_key, sample_data in retrieved_samples.items():\n",
    "        original_transcript = sample_data[\"original_transcript\"]\n",
    "        asr_transcript = sample_data[\"asr_transcript\"]\n",
    "        \n",
    "        print(f\"Generating IPA for {sample_key} with {model_name}\")\n",
    "        try:\n",
    "            original_ipa = (\n",
    "                generate_ipa(model, original_transcript)\n",
    "                if original_transcript != \"No original transcription available\"\n",
    "                else \"N/A\"\n",
    "            )\n",
    "            asr_ipa = generate_ipa(model, asr_transcript)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating IPA for {sample_key} with {model_name}: {str(e)}\")\n",
    "            original_ipa = \"Error\"\n",
    "            asr_ipa = \"Error\"\n",
    "        \n",
    "        # Add IPA to the sample data\n",
    "        sample_data[f\"original_ipa_{model_name}\"] = original_ipa\n",
    "        sample_data[f\"asr_ipa_{model_name}\"] = asr_ipa\n",
    "        \n",
    "        print(f\"Processed {sample_key} with {model_name}:\")\n",
    "        print(f\"  Original Transcript: {original_transcript}\")\n",
    "        print(f\"  ASR Transcript: {asr_transcript}\")\n",
    "        print(f\"  Original IPA ({model_name}): {original_ipa}\")\n",
    "        print(f\"  ASR IPA ({model_name}): {asr_ipa}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Clean up the current model\n",
    "    print(f\"Unloading {model_name}\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(f\"{model_name} unloaded successfully.\")\n",
    "\n",
    "# Save the updated JSON\n",
    "output_json_file = \"transcription_results_with_ipa.json\"\n",
    "print(f\"\\nSaving updated JSON to {output_json_file}\")\n",
    "json_output = json.dumps(retrieved_samples, indent=4,ensure_ascii=False)\n",
    "with open(output_json_file, \"w\") as f:\n",
    "    f.write(json_output)\n",
    "print(f\"Updated results saved to '{output_json_file}'\")\n",
    "\n",
    "# Final memory cleanup\n",
    "gc.collect()\n",
    "print(\"Final memory cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4802e280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 21:55:21.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 21:55:21.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 21:55:21.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/llama-chat-3.1-q8.gguf\n",
      "Loading LLaMA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "2025-04-09 21:56:08.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-09 21:56:08.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /media/cairuser1/b916a6fe-2106-41d4-98be-bbdd1d3bcb16/model/llama-chat-3.1-q8.gguf\n"
     ]
    }
   ],
   "source": [
    "ipas = json.load(open('./transcription_results_with_ipa.json','r'))\n",
    "model = load_model(model_path=model_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "363fed20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IPA Mismatches': 1, 'Normalized Error Rate': 0.1, 'Similarity Score': 0.9}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "# Example IPA strings\n",
    "ipa_original = \"dÉªs Éªz É™ tÉ›st\"\n",
    "ipa_transcript = \"Ã°Éªs Éªz É™ tÉ›st\"\n",
    "\n",
    "def calculate_distance(ipa_original,ipa_transcript):\n",
    "    # Step 1: Clean up and remove spaces if needed\n",
    "    ipa_original_clean = ipa_original.replace(\" \", \"\")\n",
    "    ipa_transcript_clean = ipa_transcript.replace(\" \", \"\")\n",
    "\n",
    "    # Step 2: Calculate raw Levenshtein distance (number of edits)\n",
    "    distance = Levenshtein.distance(ipa_original_clean, ipa_transcript_clean)\n",
    "\n",
    "    # Step 3: Normalize (optional, to get a percentage)\n",
    "    normalized_error = distance / max(len(ipa_original_clean), 1)\n",
    "\n",
    "    # Step 4: Similarity Score (optional)\n",
    "    similarity_score = 1 - normalized_error\n",
    "\n",
    "    # Output\n",
    "    return {\"IPA Mismatches\": distance,\"Normalized Error Rate\": normalized_error,\"Similarity Score\": similarity_score}\n",
    "calculate_distance(ipa_original,ipa_transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e34eb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ipas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ipas_Evaluated \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[43mipas\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m     sample \u001b[38;5;241m=\u001b[39m ipas[key]\n\u001b[1;32m      4\u001b[0m     evaluation \u001b[38;5;241m=\u001b[39m evaluate_transcriptions(\n\u001b[1;32m      5\u001b[0m         model,\n\u001b[1;32m      6\u001b[0m         sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_transcript\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \n\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ipas' is not defined"
     ]
    }
   ],
   "source": [
    "ipas = json.load(open('./transcription_results_with_ipa.json','r'))\n",
    "model = load_model(model_path=model_paths[0])\n",
    "ipas_Evaluated = {}\n",
    "for key in ipas.keys():\n",
    "    sample = ipas[key]\n",
    "    evaluation = evaluate_transcriptions(\n",
    "        model,\n",
    "        sample['original_transcript'],\n",
    "        sample['asr_transcript'],\n",
    "        [sample['original_ipa_model_1'],sample['original_ipa_model_2'],sample['original_ipa_model_3']],\n",
    "        [sample['asr_ipa_model_1'],sample['asr_ipa_model_2'],sample['asr_ipa_model_3']],\n",
    "        \n",
    "    )\n",
    "    sample['evaluated_ipa'] = evaluation \n",
    "    try:\n",
    "        sample['evaluated_ipa']['error']=calculate_distance(evaluation['best_ipa_original'],evaluation['best_ipa_transcript'])\n",
    "    except:\n",
    "        print(\"parsing failed\")\n",
    "        sample['evaluated_ipa']['error']= {\"IPA Mismatches\": None,\"Normalized Error Rate\": None,\"Similarity Score\": None}\n",
    "    ipas_Evaluated[key] = sample\n",
    "    \n",
    "json.dump(ipas_Evaluated,open('transcription_results_with_evaluated_ipa.json','w'),ensure_ascii=False,indent= 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02da97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542c64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('transcription_results_with_evaluated_ipa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6dead10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('transcription_results_with_evaluated_ipa.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd327bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "status = {}\n",
    "for idx,key in enumerate(data.keys()):\n",
    "    status[f\"{idx}\"] = []\n",
    "    status[f\"{idx}\"].append(data[key]['speech_status'])\n",
    "    status[f\"{idx}\"].append(data[key]['evaluated_ipa']['error']['IPA Mismatches'])\n",
    "    status[f\"{idx}\"].append(data[key]['evaluated_ipa']['error']['Similarity Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "886a7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(status).T\n",
    "df.columns = [\"speech_status\",\"IPA Mismatches\",\"Similarity Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88c60ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech_status</th>\n",
       "      <th>IPA Mismatches</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>5</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>27</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>26</td>\n",
       "      <td>0.16129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>14</td>\n",
       "      <td>0.621622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dysarthria</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>healthy</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>healthy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>healthy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>healthy</td>\n",
       "      <td>10</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speech_status IPA Mismatches Similarity Score\n",
       "0     dysarthria              5         0.285714\n",
       "1     dysarthria             27         0.181818\n",
       "2     dysarthria              1            0.875\n",
       "3     dysarthria              0              1.0\n",
       "4     dysarthria              0              1.0\n",
       "5     dysarthria              1              0.8\n",
       "6     dysarthria              0              1.0\n",
       "7     dysarthria             26          0.16129\n",
       "8     dysarthria             14         0.621622\n",
       "9     dysarthria              0              1.0\n",
       "10       healthy              3              0.5\n",
       "11       healthy              1              0.8\n",
       "12       healthy              2         0.666667\n",
       "13       healthy              1              0.8\n",
       "14       healthy              1             0.75\n",
       "15       healthy              0              1.0\n",
       "16       healthy              1         0.833333\n",
       "17       healthy              0              1.0\n",
       "18       healthy             10         0.615385\n",
       "19       healthy              0              1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edcec7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPA Mismatches</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dysarthria</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.692544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthy</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.796538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              IPA Mismatches Similarity Score\n",
       "speech_status                                \n",
       "dysarthria               7.4         0.692544\n",
       "healthy                  1.9         0.796538"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('speech_status').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f0012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
